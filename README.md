ğŸŒ Analyse Big Data & Machine Learning des cas COVID-19 avec PySpark  
Ce projet sâ€™inscrit dans un contexte de traitement massif de donnÃ©es sur les cas confirmÃ©s de COVID-19 Ã  lâ€™Ã©chelle mondiale. Il met en Å“uvre les techniques de Machine Learning et de traitement distribuÃ© avec Apache Spark (via PySpark), afin de rÃ©pondre Ã  des requÃªtes analytiques complexes Ã  partir dâ€™un jeu de donnÃ©es volumineux et cumulatif.

---

ğŸ§  Objectifs  
- Appliquer des techniques de Machine Learning sur des donnÃ©es Big Data  
- Utiliser PySpark pour le traitement parallÃ¨le et distribuÃ©  
- RÃ©pondre Ã  3 requÃªtes analytiques :
  1. Moyenne quotidienne des cas confirmÃ©s par pays et par mois  
  2. Moyenne, Ã©cart-type, min et max des cas par semaine et par continent  
  3. Clustering mensuel des pays les plus touchÃ©s avec **K-means**

---

ğŸ—ƒï¸ DonnÃ©es utilisÃ©es  
- DonnÃ©es COVID-19 cumulÃ©es (01/2020 Ã  03/2023)  
- 289 lignes Ã— 1147 colonnes (dates, pays, coordonnÃ©es gÃ©ographiques)  
- PrÃ©traitement :
  - Transformation du format large â†’ long (via `unpivot`)  
  - Ajout de colonnes : `Date`, `Continent`, `Year`, `Month`, `Week`, `Positive Cases`  
  - Nettoyage : suppression des lignes de croisiÃ¨res, traitement des incohÃ©rences temporelles

---

ğŸ› ï¸ MÃ©thodologie  
- **Nettoyage de donnÃ©es** : suppression des valeurs manquantes ou erronÃ©es (coordonnÃ©es nulles, valeurs nÃ©gatives, etc.)  
- **Feature engineering** :
  - Colonnes temporelles (`Year`, `Month`, `Week`, etc.)
  - Calcul des cas positifs journaliers
  - Attribution des continents Ã  chaque pays  
- **Utilisation de PySpark SQL et MLlib** pour les agrÃ©gations et le clustering  
- RequÃªtes optimisÃ©es avec des fonctions Spark distribuÃ©es

---

ğŸ“Š RequÃªtes traitÃ©es

**Query 1 â€” Par pays et par mois :**  
Calcul de la moyenne quotidienne des cas confirmÃ©s â†’ reprÃ©sentation par histogrammes pour 8 pays clÃ©s (2020â€“2021)

**Query 2 â€” Par continent et par semaine :**  
Calcul des statistiques (moyenne, min, max, Ã©cart-type) pour les 100 pays les plus touchÃ©s â†’ reprÃ©sentation par courbes

**Query 3 â€” Clustering K-means mensuel :**  
Segmentation des 50 pays les plus affectÃ©s chaque mois â†’ visualisation sur cartes du monde, regroupÃ©s par couleur (cluster)

---

â±ï¸ Temps de traitement
- PrÃ©traitement complet : 99 sec  
- Query 1 : 0.14 sec (agrÃ©gations simples)  
- Query 2 : 0.30 sec (rÃ©gression linÃ©aire incluse)  
- Query 3 : 170.8 sec (calculs + clustering)  
> âš¡ Le projet dÃ©montre la puissance de Spark dans le traitement efficace de grands volumes de donnÃ©es.

---

ğŸ“š Technologies utilisÃ©es  
- ğŸ”¥ Apache Spark (PySpark)  
- ğŸ Python, Pandas  
- ğŸ“¦ Scikit-learn (rÃ©gression linÃ©aire)  
- ğŸ§ª MLlib (clustering K-means)  
- ğŸ“Š Matplotlib, Seaborn  
- ğŸ“ Jupyter Notebook

---

ğŸ“ Conclusion  
Ce projet dÃ©montre la pertinence de Spark pour traiter efficacement des donnÃ©es de santÃ© massives et complexes.  
Les rÃ©sultats des 3 requÃªtes sont conformes aux tendances rÃ©elles de la pandÃ©mie de COVID-19 entre 2020 et 2023.  
Une prochaine Ã©tape serait lâ€™automatisation de ces analyses avec des donnÃ©es mises Ã  jour quotidiennement, ou la comparaison des temps de traitement avec une approche basÃ©e uniquement sur Pandas.

